{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Logistic classifier for digit regonition.\n",
    "\n",
    "In this example, images of handwritten numbers, are identified as to which digit each is.    \n",
    "\n",
    "Logistic classification is based upon first optaining scores from a linear model.\n",
    "\n",
    "The model will calculate a score for \n",
    "\n",
    "$$ y = WX + b $$\n",
    "\n",
    "X is our input vector.  W is a matrix of weights and b is the bias.\n",
    "\n",
    "Durning the learning phase y is the labels that have been provided with the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the parameter `one_hot=True` gives labels that are \"one hot\" encoded.  The label for each image is a 10 element array (one element for each digit).  For each sample the correct array element has a value of 1 and all others are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(43120000,) dtype=float32>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(mnist.train.images, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35609257.,    524055.,    471209.,    401079.,    409263.,\n",
       "           461877.,    430081.,    475686.,    560682.,   3776811.]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(tf.reshape(mnist.train.images, [-1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1276b0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note about the MNIST dataset is that it is not well distributed.  If there is a good way to recondition the data, it should be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variable matrix for W and a variable vector for b.  \n",
    "# These are what will be optimized by the learning phase.\n",
    "\n",
    "# W = tf.Variable(tf.zeros([784, 10])) # Here we created variables initialized with all zero values\n",
    "# b = tf.Variable(tf.zeros([10])) \n",
    "\n",
    "rand_init_W = tf.random_normal([784, 10], 0, 0.2) # Initialize with random values from a normal distribution.\n",
    "rand_init_b = tf.random_normal([10], 0, 0.2)\n",
    "\n",
    "W = tf.Variable(rand_init_W)\n",
    "b = tf.Variable(rand_init_b)\n",
    "\n",
    "# Create a placeholder variable for the image data\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear equation that is our model can be expressed as a matrix equation.  \n",
    "\n",
    "The score for each image is a vector given by:\n",
    "\n",
    "$$ evidence_i = \\sum_j W_{ij}x_{j} + b_{ij} $$\n",
    "\n",
    "Here i is the digit outcome (0-9).  x is a vector of the image data (the image data is flatten to a 1D array in this example).\n",
    "\n",
    "To do the evidence sum for all digits at once, do a matrix multiply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are building the model before initializing the data into it.\n",
    "\n",
    "evidence = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "order of operations matters in matrix ops (but x is a vector, does this matter here (have students check)).  Wether the vector is row or column matters here.  (This will be covered eariler, so this question should be review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The softmax distribution converts the scores into a proability (where the sum of the probabilities is one)\n",
    "\n",
    "y = tf.nn.softmax(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are the y values in the first equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This variable is for the correct values of the training set (the labels)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need an error between predicted and actual values to minimize.  The cross entropy evaluates the distance between the predicted and correct vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce_sum term calculates the .  Using mean in the second term rather then sum, keeps the value of the cross entropy smaller which is better for numerial stability when optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alturnative calculation of cross entropy is the softmax_cross_entropy_with_logits function.  This impliments the softmax and cross entropy calculations together in a more numerially stable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are randomly selecting 100 images and their labels to train on and repeating 1000 times.  This is an example of stochastic gradient descent.  It would be very computational expensive (and less numerically stable) to do gradient desent on 55000 samples.\n",
    "\n",
    "Exercise:  Test the effect of batch size on the on the training effectivness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.argmax function gives the index of the highest value in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8816\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% - 1000, 100 sample batches\n",
    "no improvement for 200 sample batches\n",
    "no significant loss for 50 sample batches\n",
    "\n",
    "86% - 100 100 sample batches\n",
    "57% - 10 100 sample batches\n",
    "\n",
    "90% is not a very good predictor for clean data like MNIST\n",
    "\n",
    "The number of training steps effects the training more than the batch size in stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improving results with a convolutional neural network (CNN)\n",
    "\n",
    "In this network we will use a different activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
